[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blog posts about Maths, AI/ML",
    "section": "",
    "text": "Collaborative Filtering\n\n\n\nrecommendation\n\ncode\n\n\n\n\n\n\n\n\n\nAug 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing\n\n\n\nNLP\n\ntransfer learning\n\ncode\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/natural-language-processing/index.html",
    "href": "posts/natural-language-processing/index.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "In this notebook, we are going to deep dive into natural language processing (NLP) using Deep Learning (info). Relying on the pretrained language model, we are going to fine-tune it to classify the reviews, and it works as sentiment analysis.\nBased on a language model which has been trained to guess what the next word in the text is, we will apply transfer learning method for this NLP task.\n\n\n\n\n\n\nFigure 1: Transfer learning workflow for movie classifier\n\n\n\nAs shown in Figure 1, we will start with the Wikipedia language model with a subset which we called Wikitext103. Then, we are going to create an IMDb language model which predicts the next word of a movie reviews. This intermediate learning will help us to learn about IMDb-specific kinds of words like the name of actors and directors. Afterward, we end up with our classifier."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#tokenization",
    "href": "posts/natural-language-processing/index.html#tokenization",
    "title": "Natural Language Processing",
    "section": "Tokenization",
    "text": "Tokenization\nBasically, tokenization convert the text into list of words. Firstly, we will grab our IMDb dataset and try out the tokenizer with all the text files.\n\n\nCode\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n# path.ls()\n\n\n\n\nCode\nfiles = get_text_files(path,folders=['train','test','unsup'])\n\n\nThe default English word tokenizer that FastAI used is called SpaCy which uses a sophisticated riles engine for particular words and URLs. Rather than directly using SpacyTokenizer, we are going to use WordTokenizer which always points to fastai’s current default word tokenizer.\n\n\nCode\ntxt = files[0].open().read()\ntxt[:60]\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\n\nprint(coll_repr(toks,30))\n\n\n(#365) ['While','the','premise','of','the','film','is','pretty','lame','(','Ollie','is','diagnosed','with','\"','hornophobia','\"',')',',','the','film','is','an','amiable','and','enjoyable','little','flick','.','It'...]\n\n\n\nSub-word tokenization\nIn additions to word tokenizer, sub-word tokenizer is really useful for languages which the spaces are not necessary for separations of components in a sentence (e.g: Chinese). To handle this, we will do 2 steps: - Analyze a corpus of documents to find the most commonly occurring groups of letters which form the vocab - Tokenize the corpus using this vocab of sub-word units\nFor example, we will first look into 2000 movie reviews:\n\n\nCode\ntxts = L(o.open().read() for o in files[:2000])\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nThen, the long underscore is when we replace the space and we can know where the sentences actually start and stop.\n\n\nCode\nsubword(10000)\n\n\n\n\n\n\n\n\n\n'▁Whil e ▁the ▁premise ▁of ▁the ▁film ▁is ▁pretty ▁lame ▁( O ll ie ▁is ▁diagnos ed ▁with ▁\" hor no pho b ia \") , ▁the ▁film ▁is ▁an ▁a mi able ▁and ▁enjoyable ▁little ▁flick . ▁It \\''\n\n\nIf we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence. So, there is a compromise to take into account when choosing sub-word vocab: A larger vocab means more fewer tokens per sentence which means faster training, less memory, less state for the model to remember, but it comes to the downside of larger embedding matrix and requiring more data to learn."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#numericalization",
    "href": "posts/natural-language-processing/index.html#numericalization",
    "title": "Natural Language Processing",
    "section": "Numericalization",
    "text": "Numericalization\nIn order to numericalize, we need to call setup first to create the vocab.\n\n\nCode\ntkn = Tokenizer(spacy)\ntoks300 = txts[:300].map(tkn)\ntoks300[0]\nnum = Numericalize()\nnum.setup(toks300)\ncoll_repr(num.vocab,20)\n\n\n\"(#2976) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\"\n\n\nThe results return our rule tokens first, and it is followed by word appearances, in frequency order. Once we created our Numerical object, we can use it as if it were a function.\n\n\nCode\nnums = num(toks)[:20]\nnums\n\n\nTensorText([   0,    9,  938,   14,    9,   30,   16,  173, 1227,   35,    0,\n              16,    0,   27,   23,    0,   23,   33,   10,    9])\n\n\n\n\nCode\n' '.join(num.vocab[o] for o in nums)\n\n\n'xxunk the premise of the film is pretty lame ( xxunk is xxunk with \" xxunk \" ) , the'\n\n\nNow, we have already had numerical data, we need to put them in batches for our model.\n\nBatches of texts\nRecalling the batch creation for the images when we have to reshape all the images to be same size before grouping them together in a single tensor for the efficient calculation purposes. It is a little bit different when dealing with texts because it is not desirable to resize the text length. Also, we want the model read texts in order so that it can efficiently predict what the next word is. This suggests that each new batch should begin precisely where the previous one left off.\nSo, the text stream will be cut into a certain number of batches (with batch size) with preserving the order of the tokens. Because we want the model to read continuous rows of the text.\nTo recap, at every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, that stream will be cut into a batch of fixed size consecutive mini stream. The model will read these mini streams in order and it will produce the same activation.\n\n\nCode\nnums300 = toks300.map(num)\ndl = LMDataLoader(nums300)\nx,y = first(dl)\nx.shape, y.shape\n\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\nThe batch size is 64x72. 64 is the default batch size and 72 is the default sequence length."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "href": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "title": "Natural Language Processing",
    "section": "Create a language model using DataBlock",
    "text": "Create a language model using DataBlock\nBy default, fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock.\n\n\nCode\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\nblocks=TextBlock.from_folder(path, is_lm=True),\nget_items=get_imdb, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=128, seq_len=80)"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#fine-tuning-the-language-model",
    "href": "posts/natural-language-processing/index.html#fine-tuning-the-language-model",
    "title": "Natural Language Processing",
    "section": "Fine-tuning the language model",
    "text": "Fine-tuning the language model\nThen, we are going to create a learner which is going to learn and predict the next word of a movie review. It will take the data from data loader, pretrained model (AWD_LSTM), Dropout technique and metrics into account.\n\n\nCode\nlearn = language_model_learner(\ndls_lm, AWD_LSTM, drop_mult=0.3,\nmetrics=[accuracy, Perplexity()]).to_fp16()\n\n\nThen we will do training (fit_one_cycle instead of fine_tuning) because we will be saving the intermediate model results during the training process.\n\n\nCode\nlearn.fit_one_cycle(1,2e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.005774\n3.901640\n0.300593\n49.483521\n2:38:18\n\n\n\n\n\nAfter few minutes, we got the accuracy of prediction using transfer learning which is about 29 percent. In order to intermediately save the pre-trained model, we can easily do it with PyTorch, and it will create a file in learn.path/models. Afterward, we can load the content of the file without any difficulty.\n\n\nCode\n# Option 1: Save with FastAI\n# learn.save('one_epoch_training')\n\n# Option 2: Save with PyTorch\nimport torch\nmodel_save_path = learn.path/'models'/'one_epoch_training_torch.pth'\ntorch.save(learn.model.state_dict(), model_save_path)\n# print(f\"Model saved to: {model_save_path}\")\n\n\n\n\nCode\n# Option 1: Use FastAI's load method\n# learn.load('one_epoch_training', strict=False)\n\n# Option 2: Use PyTorch to load the saved model\nimport torch\nmodel_load_path = learn.path/'models'/'one_epoch_training_torch.pth'\nstate_dict = torch.load(model_load_path, weights_only=False)\nlearn.model.load_state_dict(state_dict, strict=False)\n# print(f\"Model loaded from: {model_load_path}\")\n\n\n&lt;All keys matched successfully&gt;\n\n\nAfter loading the pre-saved model, we can unfreeze it and train it for few more epochs. Then, let’s see the improvement of the accuracy.\n\n\nCode\nlearn.unfreeze()\n\nlearn.fit_one_cycle(10,2e-3)\n\n\n/home/ldinh/perso/.venv/lib64/python3.12/site-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/home/ldinh/perso/.venv/lib64/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/home/ldinh/perso/.venv/lib64/python3.12/site-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/home/ldinh/perso/.venv/lib64/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.764500\n3.760669\n0.316729\n42.977158\n2:18:55\n\n\n1\n3.708819\n3.698286\n0.323979\n40.378021\n2:32:35\n\n\n2\n3.633408\n3.652018\n0.329161\n38.552387\n2:33:59\n\n\n3\n3.566183\n3.620933\n0.332817\n37.372433\n2:25:56\n\n\n4\n3.496074\n3.599054\n0.335281\n36.563614\n2:30:27\n\n\n5\n3.428904\n3.582709\n0.337807\n35.970840\n2:28:41\n\n\n6\n3.363692\n3.573828\n0.339434\n35.652794\n2:19:28\n\n\n7\n3.301039\n3.571042\n0.340221\n35.553623\n2:15:53\n\n\n8\n3.259397\n3.574479\n0.340391\n35.676033\n2:27:58\n\n\n9\n3.213290\n3.579186\n0.340240\n35.844360\n2:36:59\n\n\n\n\n\nThen, we save our model except the last activation function layer. To do that, we can save it with save_encoder\n\n\nCode\nlearn.save_encoder('finetuned')\n\n\nIn this step, we have fine tuned the language model. Now, we will fine tune this language model using the IMDb sentiment labels."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#text-generation",
    "href": "posts/natural-language-processing/index.html#text-generation",
    "title": "Natural Language Processing",
    "section": "Text generation",
    "text": "Text generation\nWe can self create some random words and we can create sentences and each contains 40 words and we will predict the content of those with a kind of randomization.\n\n\nCode\nTEXT = \"I liked this movie so\"\n\nN_WORDS = 40\n\nN_SENTENCES = 2\n\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see the generation of new inventing words\n\n\nCode\nprint(\"\\n\".join(preds))\n\n\ni liked this movie so much , i did n't see it in the theater because i was in my 20s . i was in the first time in the theater when i was growing up . i also did n't like the fact that\ni liked this movie so much that i enjoyed it . It is a good movie . Get the DVD out of your library . This movie is a classic for any romantic movie fan . i recommend it . i"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#creating-the-classifier-dataloaders",
    "href": "posts/natural-language-processing/index.html#creating-the-classifier-dataloaders",
    "title": "Natural Language Processing",
    "section": "Creating the classifier DataLoaders",
    "text": "Creating the classifier DataLoaders\nPreviously, we built a language model to predict the next word of a document given the pre text. Now, we are going to move to the classifier which predict the sentiment of a document.\n\n\nCode\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\n\nLet’s see some example of data set:\n\n\nCode\ndls_clas.show_batch(max_n=5)\n\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\nxxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\npos\n\n\n1\nxxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\npos\n\n\n2\nxxbos xxmaj some have praised xxunk xxmaj lost xxmaj xxunk as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n\\n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older adventure movies has been done well before , ( think xxmaj the xxmaj dirty xxmaj dozen ) but xxunk represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before ,\nneg\n\n\n3\nxxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of oatmeal . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain overconfidence on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an idyllic storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is\nneg\n\n\n4\nxxbos xxmaj berlin - born in 1942 xxmaj margarethe von xxmaj trotta was an actress and now she is a very important director and writer . xxmaj she has been described , perhaps even unfairly caricatured , as a director whose commitment to bringing a woman 's sensibility to the screen outweighs her artistic strengths . \" rosenstrasse , \" which has garnered mixed and even strange reviews ( the xxmaj new xxmaj york xxmaj times article was one of the most negatively aggressive reviews xxmaj i 've ever read in that paper ) is not a perfect film . xxmaj it is a fine movie and a testament to a rare xxunk of successful opposition to the genocidal xxmaj nazi regime by , of all peoples , generically powerless xxmaj germans demonstrating in a xxmaj berlin street . \\n\\n xxmaj co - writer von xxmaj trotta uses the actual\npos"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my blog!\nIn this site, you will find my contributions and highlights of my writings on Mathematics and Computer Science.\nIf you are interested in this field, feel free to connect with me and I would love to hear from you! See the article on contributing posts or my contact for additional details."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html",
    "href": "posts/collaborative-filtering/index.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "General context\nWhen we think about Netflix, we might have watched lots of movies that are science fiction, action, horror etc. Netflix may not know these particular properties of the films you watched, but it would be able to see that other people that watched the same movies could watch other movies that you are not watching yet. By doing recommendation approach, Netflix can recommend us the contents of the movies that we have not watched before but relevant to what we liked.\nThis approach is called collaborative filtering. The key foundation idea is that of latent factors which decides what kinds of movies you want to watch.\n\n\nData set\nIndeed, we can not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can yous, called MovieLen which contains tens millions of movies ranking.\n\n\nCode\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\n# path = untar_data(URLs.ML_100k)"
  }
]