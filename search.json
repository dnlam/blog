[
  {
    "objectID": "posts/natural-language-processing/index.html",
    "href": "posts/natural-language-processing/index.html",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "",
    "text": "In this blog, we are going to deep dive into natural language processing (NLP) using Deep Learning (info). Relying on the pretrained language model, we are going to fine-tune it to classify the reviews, which works as sentiment analysis, to categorize user reviews as bad/good ones.\nBased on a language model which has been trained before, we will apply transfer learning method for this task to transform prediction problem into classification problem.\n\n\n\n\n\n\nLanguage model\n\n\n\nIn this blog post, we refer language model as a model which predicts the next word in a sentence given the previous words. It is a self-supervised learning task, where the model learns to predict the next word in a sentence based on the context provided by the preceding words.\n\n\n\n\n\n\n\n\nFigure 1: Transfer learning workflow for movie classifier\n\n\n\nAs shown in Figure 1, we will start with the Wikipedia language model with a dataset which so-called Wikitext103 1. Then, we are going to create an IMDb language model which predicts the next word of a movie reviews. This intermediate learning will help us to learn about IMDb-specific kinds of words like the name of actors and directors. Afterward, we end up with fine-tuning the language model for classification problem to classify reviews for a movie is good/bad.\n\n\n\n\n\n\nThree-Step Transfer Learning Process\n\n\n\n\nGet Pre-trained Model: Clone Wikitext103 language model\nDomain Adaptation: Fine-tune Wikitext103 which is based on Wiki texts with IMDb movie reviews.\n\nCreate Task-Specific: Refine the fine-tuned model for sentiment classification."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#text-preprocessing",
    "href": "posts/natural-language-processing/index.html#text-preprocessing",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\nIn order to build a language model with many complexities such as different sentence lengths in long documents, we can build a neural network model to deal with that issue. We apprehended that categorical variables (words) can be used as independent variables for a neural network (using embedding matrix). Then, we could do the same thing with text.\nFirst, we concatenate all the documents in our dataset into a big long string and split it into words. Our independent variables will be the sequence of words starting with the first word and ending with the second last, and our dependent variable would be the sequence of words starting with the second word and ending with the last words.\nIn our vocab, it might exist the very common words and new words. For new words, because we don’t have any pre-knowledge, so we will just initialize the corresponding row with a random vector.\nThese above steps can be listed as below: - Tokenization: convert the text into a list of words - Numericalization: make a list of all the unique words which appear, and convert each word into a number, by looking up its index in the vocab. - Language model data loader: handle creating dependant variables - Language model: handle input list by using recurrent neural network.\n\nTokenization\nBasically, tokenization converts the text into list of words. Firstly, we will grab our IMDb dataset and try out the tokenizer with all the text files.\n\n\nCode\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n# path.ls()\n\n\n\n\nCode\nfiles = get_text_files(path,folders=['train','test','unsup'])\n\n\nThe default English word tokenizer that FastAI used is called SpaCy which uses a sophisticated rules engine for particular words and URLs. Rather than directly using SpacyTokenizer, we are going to use WordTokenizer which always points to fastai’s current default word tokenizer.\n\n\nCode\ntxt = files[0].open().read()\ntxt[:60]\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\n\nprint(coll_repr(toks,30))\n\n\n(#230) ['Cinematically',',','this','film','stinks','.','So','does','a','lot','of','the','acting','.','But','I','&lt;','br','/&gt;&lt;br',\"/&gt;don't\",'care','.','If','there','is','a','strong','representation','of','what'...]\n\n\n\nSub-word tokenization\nIn additions to word tokenizer, sub-word tokenizer is really useful for languages which the spaces are not necessary for separations of components in a sentence (e.g: Chinese). To handle this, we will do 2 steps: - Analyze a corpus of documents to find the most commonly occurring groups of letters which form the vocab - Tokenize the corpus using this vocab of sub-word units\nFor example, we will first look into 2000 movie reviews:\n\n\nCode\ntxts = L(o.open().read() for o in files[:2000])\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nThen, the long underscore is when we replace the space and we can know where the sentences actually start and stop.\n\n\nCode\nsubword(10000)\n\n\n\n\n\n\n\n\n\n\"▁Cinema t ically , ▁this ▁film ▁st ink s . ▁So ▁does ▁a ▁lot ▁of ▁the ▁acting . ▁But ▁I &lt; br ▁/&gt; &lt; br ▁/&gt; don ' t ▁care . ▁If ▁there ▁is ▁a ▁strong ▁representation ▁of ▁what ▁the\"\n\n\nIf we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence. So, there is a compromise to take into account when choosing sub-word vocab: A larger vocab means more fewer tokens per sentence which means faster training, less memory, less state for the model to remember, but it comes to the downside of larger embedding matrix and requiring more data to learn.\n\n\n\nNumericalization\nIn order to numericalize the tokens, we need to call setup first to create the vocab.\n\n\nCode\ntkn = Tokenizer(spacy)\ntoks300 = txts[:300].map(tkn)\ntoks300[0]\nnum = Numericalize()\nnum.setup(toks300)\ncoll_repr(num.vocab,20)\n\n\n\"(#2952) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','in','it','i'...]\"\n\n\nThe results return our rule tokens first, and it is followed by word appearances, in frequency order. Once we created our Numerical object, we can use it as if it were a function.\n\n\nCode\nnums = num(toks)[:20]\nnums\n\n\nTensorText([  0,  11,  21,  25,   0,  10,   0,  83,  13, 147,  14,   9, 154,\n             10,   0,   0,   0,   0,   0,   0])\n\n\n\n\nCode\n' '.join(num.vocab[o] for o in nums)\n\n\n'xxunk , this film xxunk . xxunk does a lot of the acting . xxunk xxunk xxunk xxunk xxunk xxunk'\n\n\nNow, we have already had numerical data, we need to put them in batches for our model.\n\n\nProcessing Batches of texts\nRecalling the batch creation for the images when we have to reshape all the images to be same size before grouping them together in a single tensor for the efficient calculation purposes. It is a little bit different when dealing with texts because it is not desirable to resize the text length. Also, we want the model read texts in order so that it can efficiently predict what the next word is. This suggests that each new batch should begin precisely where the previous one left off.\nSo, the text stream will be cut into a certain number of batches (with batch size) with preserving the order of the tokens. Because we want the model to read continuous rows of the text.\nTo recap, at every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, that stream will be cut into a batch of fixed size consecutive mini stream. The model will read these mini streams in order and it will produce the same activation.\n\n\nCode\nnums300 = toks300.map(num)\ndl = LMDataLoader(nums300)\nx,y = first(dl)\nx.shape, y.shape\n\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\nThe batch size is 64x72. 64 is the default batch size and 72 is the default sequence length."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "href": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Create a language model using DataBlock",
    "text": "Create a language model using DataBlock\nBy default, fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock.\n\n\nCode\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\nblocks=TextBlock.from_folder(path, is_lm=True),\nget_items=get_imdb, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=128, seq_len=80)"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#fine-tuning-the-model",
    "href": "posts/natural-language-processing/index.html#fine-tuning-the-model",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Fine-tuning the model",
    "text": "Fine-tuning the model\nso far, we have finetuned encoder, which stores the trained parameter weights from previous step. Now, we are going to create a learner that load the finetuned encoder for fine-tuning. Then, we are going to fine-tune it over several epoch.\n\n\nCode\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n                                metrics=accuracy).to_fp16()\n\n\n\n\nCode\nlearn.load_encoder('finetuned')\n\n\n&lt;fastai.text.learner.TextLearner at 0x7f17aa2c3010&gt;\n\n\nAs we are training a classification task, we only need to unfreeze several last layers of the encoder instead of unfreezing all the layers. By fitting the last layers first, we can adapt the model to the specific task without losing the general language understanding captured in the earlier layers. The result show that we achieve around 93 % accuracy, just with one cycle fitting.\n\n\nCode\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232339\n0.169032\n0.935000\n00:29\n\n\n\n\n\nWe can also further unfreeze more layers and do training to see if the accuracy improves or not. It shows that the accuracy results have improved from 93% to approximately 94%.\n\n\nCode\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.204471\n0.154350\n0.941800\n00:38\n\n\n\n\n\nAs we can observe, the accuracy has improved with the unfreezing of more layers. Now, we unfreeze all layers and do some training. The accuracy results are further improved to approximately 95%.\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.172045\n0.146765\n0.945280\n00:50\n\n\n1\n0.154358\n0.146338\n0.945200\n00:50"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#key-points",
    "href": "posts/natural-language-processing/index.html#key-points",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Key points",
    "text": "Key points\n\nWe started with the Wikitext103 pretrained model and fine-tuned it on IMDb movie reviews, achieving approximately 35% accuracy in next-word prediction after 10 epochs of training.\nWe implemented a text preprocessing pipeline for a language model including:\n\nTokenization: Converting raw text into structured word sequences using SpaCy\nNumericalization: Mapping words to numerical representations for neural network processing\nBatch Creation: Organizing sequential text data for efficient model training\n\nWe achieved final sentiment classification through progressive layer unfreezing of fine-tuned model."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#technical-insights",
    "href": "posts/natural-language-processing/index.html#technical-insights",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Technical Insights",
    "text": "Technical Insights\n\n\n\n\n\n\nKey Technical Learnings\n\n\n\n\nModel Persistence: Demonstrated both FastAI and PyTorch approaches for saving and loading model states\nProgressive Training: Used gradual unfreezing technique to optimize classification performance"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#future-directions",
    "href": "posts/natural-language-processing/index.html#future-directions",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Future Directions",
    "text": "Future Directions\nThis foundation opens several avenues for enhancement:\n\nModel Architecture: Experiment with transformer-based models (BERT, GPT)\nDataset Expansion: Include additional movie review sources for robustness\nMulti-class Classification: Extend beyond binary sentiment to rating prediction\nReal-time Deployment: Package the model for production sentiment analysis"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#final-thoughts",
    "href": "posts/natural-language-processing/index.html#final-thoughts",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis project demonstrates the power of transfer learning in NLP, showing how pretrained language models can be effectively adapted for specific downstream tasks. The combination of FastAI’s high-level API with PyTorch’s flexibility provides an excellent framework for both experimentation and production deployment."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#footnotes",
    "href": "posts/natural-language-processing/index.html#footnotes",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.↩︎"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html",
    "href": "posts/collaborative-filtering/index.html",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "",
    "text": "In this blog, we are going to dive into collaborative filtering which is a part of recommendation algorithms used in Netflix. Netflix may not know these particular properties of the films you watched, but it would be able to see that other people that watched the same movies could watch other movies that you are not watching yet. By applying this technique, Netflix can recommend us the contents of the movies that we have not watched before but relevant to what we liked, which the others too.\nCollaborative Filtering is a recommendation system that recommends items to users based on how other users with similar preferences and behaviors have interacted with the same item. It is based on the principle that similar users share similar interests and behaviors. Different to content-based filtering, which recommends items based on the features of the items themselves, collaborative filtering relies on the collective behavior of users to make recommendations.\nThe key idea of content-based filtering is based on latent factors which decides what kinds of movies you want to watch. # How does it work? To recommend items to users based on their past behavior and preferences, collaborative filtering first needs to find similar users. It does this by analyzing user-item interactions and identifying patterns in user behavior. Then, it predicts the ratings of items that are not yet rated by the user. To do so, we need to deal with the following questions:\n\nHow to measure user similarity?\nGiven that the users behaviors are similar, how to give the rating for an item based on the ratings of others users?\nHow to measure the accuracy of the rating since we don’t have the ground truth for the unseen items?\n\nTo deal with two former questions, Collaborative filtering uses techniques such as user-item interactions1, similarity measures2, and matrix factorization3.\nTo measure the accuracy of the rating, we can use metrics such as root mean square error (RMSE) or mean absolute error (MAE) to evaluate the performance of the recommendation system. RMSE predict ratings for a test dataset of user-item pairs whose rating values are already known. The difference between the known value and the predicted value would be the error. Square all the error values for the test set, find the average (or mean), and then take the square root of that average to get the RMSE. On the other hand, MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#memory-based-method",
    "href": "posts/collaborative-filtering/index.html#memory-based-method",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Memory-based method",
    "text": "Memory-based method\nMemory-based methods are based on statistical techniques to find similar users or items. It would find the rating \\(\\mathbf{R}\\) for user \\(\\mathbf{U}\\) and item \\(\\mathbf{I}\\) by:\n\nFinding similar users as user \\(\\mathbf{U}\\) based on their ratings for item \\(\\mathbf{I}\\) that they have rated.\nCalculate rating \\(\\mathbf{R}\\) based as a weighted sum of the ratings given by similar users to item \\(\\mathbf{I}\\).\n\n\nFinding similar users with similarity scores\nFor example, there are 4 user \\(u_1, u_2, u_3, u_4\\) \\(\\in \\mathbf{U}\\) have rated two movies \\(i_1, i_2\\) \\(\\in \\mathbf{I}\\) as follows:\n\n\n\n\n\n\n\n\n\nTo define if the preferences of two users are similar, we can use similarity scores between users or items. The most common similarity measures used in memory-based methods are:\n\nCosine similarity: Measures the cosine of the angle between two vectors in a multi-dimensional space. It is commonly used to measure the similarity between users or items based on their ratings.\nPearson correlation: Measures the linear correlation between two variables. It is often used to find users with similar tastes by comparing their rating patterns.\nJaccard similarity: Measures the similarity between two sets by comparing the size of their intersection to the size of their union. It is useful for finding similar items based on user co-occurrence.\n\nTo calculate similarity using Cosine similarity, we need a function that returns a higher similarity or smaller distance for a lower angle and a lower similarity or larger distance for a higher angle. The cosine of an angle is a function that decreases from 1 to -1 as the angle increases from 0 to 180.\n\n\n\n\n\n\nCentered Cosine Similarity\n\n\n\nIn practice, we center the ratings by subtracting the mean rating of each user from their ratings. This helps to account for individual user biases (e.g., some users may rate all movies higher or lower than others, but they are considered as having the same preferences). The centered cosine similarity is then calculated using these mean-centered ratings. This approach is also used when there are a lot of missing values in the user rating vectors, and we need to place a common value to fill up the missing values.\n\n\n\n\nCalculate Rating\nThere are multiple way to predict the rating of an item \\(\\mathbf{I}\\) for a user \\(\\mathbf{U}\\), one simple way is to use the average of the ratings for that item given by similar users. The formula for calculating the predicted rating \\(\\hat{R}_{U,I}\\) for user \\(\\mathbf{U}\\) and item \\(\\mathbf{I}\\) is: \\[\n\\mathbf{R_U} = \\sum_{u =1}^n \\frac{R_{u,I}}{n}\n\\] This formula shows that the average rating given by the n similar users is equal to the sum of the ratings given by them divided by the number of similar users, which is n.\nTo weight higher score for the users that are more similar to user \\(u\\), we can use the following formula:\n\\[\n\\mathbf{R_U} = \\frac{\\sum_{u =1}^n R_{u,I}*S_U}{\\sum_{u =1}^n S_U}\n\\] where the similar factor \\(S_u\\), which would act as weights, should be the inverse of the distance discussed above because less distance implies higher similarity. For example, you can subtract the cosine distance from 1 to get cosine similarity.\n\n\n\n\n\n\nuser-user vs item-item collaborative filtering\n\n\n\nThe techniques, where the rating matrix is used to find similar users based on the ratings they give, is called user-based or user-user collaborative filtering. If you use the rating matrix to find similar items based on the ratings given to them by users, then the approach is called item-based or item-item collaborative filtering, which is developped by Amazon."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#model-based-method",
    "href": "posts/collaborative-filtering/index.html#model-based-method",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Model-based method",
    "text": "Model-based method\nThe model-based collaborative filtering method learns a model from the user-item interactions and uses this model to make predictions. This approach can capture complex patterns in the data and is often more scalable than memory-based methods. Some common model-based techniques include:\n\nMatrix Factorization: This technique decomposes the user-item interaction matrix into lower-dimensional matrices representing latent factors for users and items. The most popular matrix factorization method is Singular Value Decomposition (SVD).\nDeep Learning: Neural networks can be used to learn complex representations of users and items. Techniques like autoencoders and recurrent neural networks (RNNs) have been applied to collaborative filtering tasks.\n\n\nMatrix Factorization\nWhen we have a large user-item matrix with many missing values, matrix factorization techniques can help us fill in the gaps. The idea is to factor the original matrix into two lower-dimensional matrices: one representing users and the other representing items. This allows us to capture latent factors that explain the observed ratings.\n\n\n\n\n\n\nFigure 1: Matrix Factorization Example (Source: Google Developers)\n\n\n\nAs an example Figure 1, a user-item matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) can be factorized into two lower-dimensional matrices (embedding vectors) \\(U \\in \\mathbb{R}^{m \\times k}\\) and \\(V \\in \\mathbb{R}^{k \\times n}\\), where \\(k\\) is the number of features or latent factors, \\(m\\) is the number of users, and \\(n\\) is the number of items.\nUsing embeddings for users and items, we can represent each user or item as a dense vector in a continuous space, capturing their latent characteristics via approximating the original user-item matrix \\(A\\) as follows: \\(A=U\\times V^T\\)\nTo do that, we need to learn the user and item embeddings from the data. It is achieved by minimizing the difference between the original matrix and the product of the two lower-dimensional matrices. \\[\n\\min_{U,V} ||A - U \\cdot V^T||_F^2\n\\] This corresponds to minimizing the squared Frobenius distance between the original matrix and the reconstructed matrix.\nInstead of using techniques like Singular Value Decomposition (SVD) to solve the problem, which is not favorable when matrix \\(A\\) is sparse, stochastic gradient descent (SGD) is a more feasible candidate.\n\n\nDeep-Learning method\nSome disadvantages of matrix factorization for collaborative filtering are:\n\nRelevancy of recommended item: By using dot product as a similarity measure, popular items are generally recommended for everyone, regardless their different preferences and behavior.\nLatents features are learned within user-item training set, and can not be captured beyond that.\n\nDeep neural network can deal with these problems by incorporating item features and user features.\n\n\n\n\n\n\nFigure 2: Recommendation with DNN(Source: Google Developers)\n\n\n\nFigure Figure 2 shows a deep neural network architecture for collaborative filtering. The input layer consists of user and item features, which are then passed through several hidden layers to learn complex representations. The output layer use soft-max to illustrate the probability of a user interacting with an item."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#data-preparation",
    "href": "posts/collaborative-filtering/index.html#data-preparation",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Data preparation",
    "text": "Data preparation\nIndeed, we can not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can yous, called MovieLen which contains tens millions of movies ranking."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#model-training",
    "href": "posts/collaborative-filtering/index.html#model-training",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Model Training",
    "text": "Model Training\nTo calculate the result of each user-item interaction, we look for the index of the item in the item latent factor matrix and the index of user in the user latent factor matrix. Then, we perform the dot product between the latent vectors.\nFurthermore, to capture user positive/negative feedback in their recommendations than others, and some movies are just plain better or worse than others, we can add bias terms for users and items.\n\n\nCode\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors) \n        self.user_bias = Embedding(n_users, 1) \n        self.movie_factors = Embedding(n_movies, n_factors) \n        self.movie_bias = Embedding(n_movies, 1) \n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) \n        return sigmoid_range(res, *self.y_range)\n\n\nor\n\n\nCode\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz) \n        self.item_factors = Embedding(*item_sz) \n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act), \n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n\n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1)) \n        return sigmoid_range(x, *self.y_range)\nembs = get_emb_sz(dls)\nmodel = CollabNN(*embs)\n\n\nThen, we can train the model by first creating data batches using DataLoader. We will use the CollabDataLoaders class from FastAI to create our data loaders.\n\n\nCode\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\nn_users  = len(dls.classes['user']) \nn_movies = len(dls.classes['title'])\n\n\nLet try to train the model which is based on DotProductBias in 5 epochs with learning rate of 0.005.\n\n\nCode\nmodel = DotProductBias(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.832946\n0.948861\n00:04\n\n\n1\n0.584821\n0.928112\n00:04\n\n\n2\n0.407661\n0.949344\n00:04\n\n\n3\n0.320535\n0.960816\n00:04\n\n\n4\n0.288325\n0.960388\n00:04\n\n\n\n\n\nAs we can see from the training results, while training loss reduces with each epoch, the validation loss increases, indicating overfitting. It is obvious that the model is not generalizing well to unseen data. To tackle with this issue, we can apply regularization techniques such as weight decay.\n\n\n\n\n\n\nRegularization Techniques\n\n\n\n\nWeight Decay: This technique adds a penalty on the size of the weights to the loss function, discouraging overly complex models.\nDropout: Randomly dropping units during training to prevent co-adaptation.\nEarly Stopping: Monitoring validation loss and stopping training when it starts to increase.\n\n\n\nIn Weight Decay, limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. The updates of parameters are as follows: parameters.grad += wd * 2 * parameter\n\n\nCode\nmodel = DotProductBias(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.905391\n0.980552\n00:04\n\n\n1\n0.680553\n0.903055\n00:04\n\n\n2\n0.508638\n0.884937\n00:04\n\n\n3\n0.458852\n0.867883\n00:04\n\n\n4\n0.449187\n0.862520\n00:04\n\n\n\n\n\nAs we can see, the training is much better since overfitting has been reduced."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#model-testing",
    "href": "posts/collaborative-filtering/index.html#model-testing",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Model Testing",
    "text": "Model Testing\nOnce the model has been trained, we can use that model to find the similarity between movies. For example, We can use this to find the most similar movie to Silence of the Lambs using cosine similarity:\n\n\nCode\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) \nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n\n'Ayn Rand: A Sense of Life (1997)'"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#key-points",
    "href": "posts/collaborative-filtering/index.html#key-points",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Key points",
    "text": "Key points\n\nCollaborative filtering relies on user-item interactions to make recommendations.\nMatrix factorization techniques, such as SVD and neural collaborative filtering, can effectively capture latent factors.\nRegularization techniques are essential to prevent overfitting and improve model generalization."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#technical-insights",
    "href": "posts/collaborative-filtering/index.html#technical-insights",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Technical Insights",
    "text": "Technical Insights\n\n\n\n\n\n\nKey Technical Learnings\n\n\n\n\nRegularization Impact: Weight decay (0.1) dramatically improved validation performance, reducing overfitting in the collaborative filtering model.\nModel Architecture: The use of a neural network with embedding layers allowed for better representation of users and items, capturing complex interactions.\nSimilarity Computation: Leveraging cosine similarity enabled effective identification of similar items, enhancing the recommendation process."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#future-directions",
    "href": "posts/collaborative-filtering/index.html#future-directions",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Future Directions",
    "text": "Future Directions\n\nHybrid Approaches: Combining collaborative filtering with content-based methods could enhance recommendation quality\nAdvanced Neural Architectures: Exploring transformer-based models for capturing sequential patterns in user behavior\nCold Start Problem: Collaborative filtering can lead to some problems like cold start for new items that are added to the list. Until someone rates them, they don’t get recommended."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#final-thoughts",
    "href": "posts/collaborative-filtering/index.html#final-thoughts",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nCollaborative filtering continues to evolve with deep learning approaches that capture increasingly complex patterns in user-item interactions. For practical applications, the balance between model complexity, computational efficiency, and recommendation quality remains crucial. As recommendation systems become more integrated into our digital experiences, the techniques explored in this blog will continue to play a vital role in connecting users with relevant content.\nTo know more about this, Here are the papers for further reading on collaborative filtering and other recommendation algorithms:\n\nItem Based Collaborative Filtering Recommendation Algorithms (Sarwar et al., 2001)4\nMatrix Factorization Techniques for Recommender Systems (Koren et al., 2009)5\nUsing collaborative filtering to weave an information tapestry(Goldberg et al., 1992)6"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#footnotes",
    "href": "posts/collaborative-filtering/index.html#footnotes",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHerlocker, J. L., et al. (2004). Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems, 22(1), 5-53.↩︎\nBobadilla, J., et al. (2013). Recommender systems survey. Knowledge-based systems, 46, 109-132.↩︎\nKoren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 30-37.↩︎\nSarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2001). Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web (pp. 285-295).↩︎\nKoren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 30-37.↩︎\nGoldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative filtering to weave an information tapestry. Communications of the ACM, 35(12), 61-70.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my blog!\nIn this site, you will find my contributions and highlights of my writings on Mathematics and Computer Science.\nIf you are interested in this field, feel free to connect with me and I would love to hear from you! See the article on contributing posts or my contact for additional details."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blog posts about Maths, AI/ML",
    "section": "",
    "text": "Building SoTA Convolution Neural Networks for Image Recognition\n\n\n\nCNN\n\ncode\n\n\n\n\n\n\n\n\n\nMay 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendation System with Collaborative Filtering\n\n\n\nRecommendation\n\nCollaborative Filtering\n\nCode\n\n\n\n\n\n\n\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning a Language Model for Sentiment Classification\n\n\n\nNLP\n\nLanguage Model\n\nTransfer Learning\n\nCode\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html",
    "href": "posts/convolution-neural-networks/index.html",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "",
    "text": "In this blog post, we will explore the architecture of Convolution Neural Networks (CNN) and how they have been used to achieve state-of-the-art performance in image recognition tasks. We will also discuss some of the key components of CNNs, such as convolution layers, pooling layers, and activation functions. Finally, we will look at one of the most popular CNN architectures: ResNet."
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html#convolution-layer",
    "href": "posts/convolution-neural-networks/index.html#convolution-layer",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "Convolution Layer",
    "text": "Convolution Layer\nA convolution layer applies a set of filters (i.e., kernel) to the input image to extract features. Each filter/kernel is a small matrix that is used to scan the image and extract features. The output of a convolution layer is a set of feature maps, which are the result of applying each filter to the input image.\n\n\n\n\n\n\nFigure 1: An example of kernel\n\n\n\nAs illustrated in Figure Figure 1, a 3x3 matrix kernel is applied to the input image, which is 7x7 grid. The kernel is applied to each pixel in the image, and the output is a new pixel value that is calculated by taking the dot product of the kernel and the corresponding pixels in the image. This process is repeated for each pixel in the image, resulting in a new feature map.\nLet’s take another look at how convolution works in practice. We will use the im3 image, which is a 28x28 grayscale image of the digit 3 from the MNIST dataset. We will apply a 3x3 kernel to the image to extract features.\n\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n12\n99\n91\n142\n155\n246\n182\n155\n155\n155\n155\n131\n52\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n138\n254\n254\n254\n254\n254\n254\n254\n254\n254\n254\n254\n252\n210\n122\n33\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n220\n254\n254\n254\n235\n189\n189\n189\n189\n150\n189\n205\n254\n254\n254\n75\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n35\n74\n35\n35\n25\n0\n0\n0\n0\n0\n0\n13\n224\n254\n254\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n90\n254\n254\n247\n53\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nLet’s define a kernel that detects horizontal edges in the image. The kernel is a 3x3 matrix with values that are designed to highlight horizontal edges.\n\n\nCode\nkernel = tensor([[-1., -1., -1.],\n                 [ 0.,  0.,  0.],\n                 [ 1.,  1.,  1.]]).float()\n\n\nThis kernel will detect horizontal edges in the image by emphasizing the differences between the pixel values in the top and bottom rows of the kernel, we can also change the kernel to have the row of 1s at the top and -1s at the bottom, we can detect horizontal edges that go from dark to light, putting 1s and -1s in columns versus rows give us filters that detect vertical edges.\n\n\nCode\ndef apply_kernel(row, col, kernel):\n    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()\n\n\nFor a more in-depth guide to convolution arithmetic, see the paper A guide to convolution arithmetic for deep learning (Dumoulin, 2013)1."
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html#strides-and-padding",
    "href": "posts/convolution-neural-networks/index.html#strides-and-padding",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "Strides and Padding",
    "text": "Strides and Padding\nWith convolution arithmetic, the kernel is applied to each pixel in the image, resulting in a new feature map that the dimensions are smaller than the original image. This is because the kernel cannot be applied to the pixels at the edges of the image. To address this issue, we can use two techniques: strides and padding.\nStrides refer to the number of pixels by which we move the kernel across the image. By default, the stride is set to 1, meaning we move the kernel one pixel at a time. However, we can increase the stride to reduce the size of the output feature map. For example, if we set the stride to 2, the kernel will move two pixels at a time, resulting in a smaller output feature map.\nPadding involves adding extra pixels around the edges of the image before applying the kernel. This allows us to preserve the spatial dimensions of the input image in the output feature map. There are different types of padding, such as zero-padding (adding zeros) and reflection padding (adding a mirror image of the border pixels).\n\n\n\n\n\n\nFigure 2: An example of padding with stride of 2\n\n\n\nAs illustrated in Figure 2, a 5x5 input image is padded with a 2-pixel border of zeros, resulting in a 7x7 padded image. A 4x4 kernel is then applied to the padded image with a stride of 1, resulting in a 5x5 output feature map.\nIn general, if we add a kernel of size \\(ks \\times ks\\) (\\(ks\\) is an odd number) to an input image of size \\(n \\times n\\), the neccessary padding \\(p\\) to preserve the spatial dimensions of the input image in the output feature map is given by: \\[\np = ks//2\n\\] When \\(ks\\) is even, we can use asymmetric padding, for example, if \\(ks=4\\), we can use \\(p=(ks//2, ks//2-1)\\).\nFurthermore, if we apply the kernel with a stride of \\(s\\), the output feature map will have dimensions: \\[\n\\text{output size} = (n + 2p - ks)//(s) + 1\n\\]"
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html#create-a-convolution-layer-with-pytorch",
    "href": "posts/convolution-neural-networks/index.html#create-a-convolution-layer-with-pytorch",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "Create a Convolution Layer with PyTorch",
    "text": "Create a Convolution Layer with PyTorch\nWe can create a convolution layer using PyTorch’s nn.Conv2d class. The nn.Conv2d class takes several parameters, including the number of input channels, the number of output channels, the kernel size, the stride, and the padding.\nIn this example, we create a convolution layer with 1 input channel (grayscale image), 30 output channels (feature maps), a kernel size of 3x3, a stride of 1, and padding of 1. We also apply the ReLU activation function after the first convolution layer. One interesting property to note here is that we do not need to specify the input size when creating the convolution layer because a convolution is applied over each pixel automatically.\nWhen creating cnn as above, we see that the output shape is the same as the input shape, which is (28, 28) (This is because we have used padding to preserve the spatial dimensions of the input image in the output feature map). It is not interesting for classification task since we need only single output activation per input image.\nTo deal with this, we can use several stride-2 convolution layers to reduce the spatial dimensions of the input image in the output feature map. For example, we can use two stride-2 convolution layers to reduce the spatial dimensions of the input image from (28, 28) to (7, 7), (4x4), (2x2) and then 1.\n\n\nCode\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU()) \n    return res\n\n\nThen, we create a simple cnn which consists of several convolution layers with stride of 2, kernel size of 3 to reduce the spatial dimensions of the input image in the output feature map. We also flatten the output feature map before passing it to the final classification layer.:\n\n\nCode\nsimple_cnn = sequential(\n    conv(1, 4),   # Input: 28x28 -&gt; Output: 14x14\n    conv(4, 8),  # Input: 14x14 -&gt; Output: 7x7\n    conv(8, 16), # Input: 7x7 -&gt; Output: 4x4\n    conv(16, 32), # Input: 4x4 -&gt; Output: 2x2\n    conv(32, 2, act=False), # Input: 2x2 -&gt; Output: 1x1\n    Flatten()\n)\n\n\nTo test our simple_cnn, we can train a classification model from a batch of images from the MNIST dataset to see how effective of the feature extraction it is. To do this, we build a Learner from simple_cnn and dataset dls as follows:\n\n\nCode\nlearn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\nlearn.summary()\n\n\n\n\n\n\n\n\n\nSequential (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 4 x 14 x 14    \nConv2d                                    40         True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 8 x 7 x 7      \nConv2d                                    296        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 16 x 4 x 4     \nConv2d                                    1168       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 32 x 2 x 2     \nConv2d                                    4640       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 2 x 1 x 1      \nConv2d                                    578        True      \n____________________________________________________________________________\n                     64 x 2              \nFlatten                                                        \n____________________________________________________________________________\n\nTotal params: 6,722\nTotal trainable params: 6,722\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7f2791719a80&gt;\nLoss function: &lt;function cross_entropy at 0x7f2801519120&gt;\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\nAs we can see, the output of the final Conv2D layer is 64x2x1x1, that’s why we need to flatten it before passing it to the final classification layer.\nAfterwards, let’s train the model with low learning rate and 2 epochs using fit_one_cycle function.\n\n\nCode\nlearn.fit_one_cycle(2, 1e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.065424\n0.048044\n0.984789\n00:01\n\n\n1\n0.019719\n0.021269\n0.993131\n00:01\n\n\n\n\n\nImpressive, we are able to achieve over 98% accuracy on the classification task with MNIST dataset using simple CNN architecture (built from scratch)."
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html#improving-training-stability",
    "href": "posts/convolution-neural-networks/index.html#improving-training-stability",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "Improving Training Stability",
    "text": "Improving Training Stability\nSo far, we have created a simple 2D CNN for image classification task over the MNIST dataset and achieved around 98% accuracy. In this section, we will talk about several techniques that we can use to improve the training stability and performance of our model. To make it more interesting, we will train a CNN model to recognize 10 digits from the MNIST dataset and apply several techniques to improve its performance.\n\n\n\n\n\n\n\n\n\n\nUse more activation functions\nOne simple tweak that we can apply to improve recognition accuracy is to use more activation functions in our CNN, as we need more filters to learn more complex patterns in 10-digit MNIST samples. To achieve this, we add one more activation function after each convolution layer in our simple_cnn architecture. As a result, the number of activations ends up being doubled.\nHowever, adding more activation functions can lead to a subtle (training) problem. Specifically, when we apply 3x3-pixel kernel to the first convolution layer with 4 output filters, we embed information from 9 input pixels into 4 output pixels. While doubling the number of activation functions, we have the computation of 8 output pixels from 9 input pixels. It makes neural networks more difficult to learn the features while mapping from 9 input pixels to 8 output pixels than from 9 input pixels to 4 output pixels.\nTo deal with this issue, we can increase the kernel size from 3 to 5, which allows us to embed information from 25 input pixels into 8 output pixels. This makes it easier for the neural network to learn the features while mapping from 25 input pixels to 8 output pixels.\n\n\nCode\ndef simple_cnn():\n  return sequential(\n    conv(1, 8, 5),   # Input: 28x28 -&gt; Output: 14x14\n    conv(8, 16),  # Input: 14x14 -&gt; Output: 7x7\n    conv(16, 32), # Input: 7x7 -&gt; Output: 4x4\n    conv(32, 64), # Input: 4x4 -&gt; Output: 2x2\n    conv(64, 10, act=False), # Input: 2x2 -&gt; Output: 1x1\n    Flatten()\n)\n\n\nTo train the model more quickly, we can set learning rate to 0.06 and use ActivationStats callback to monitor the activation statistics during training.\n\n\nCode\ndef fit(epochs=1):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit(epochs, 0.06)\n    return learn\n\nlearn = fit()\n\n\n/home/ec2-user/perso/.perso/lib/python3.11/site-packages/fastai/callback/core.py:71: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.745531\n0.789573\n0.738500\n00:08\n\n\n\n\n\nSurprisingly, the model is not trained at all, the accuracy is just around 10% (random guess). To findout what went wrong, we can plot the activation statistics of the first/penultimate convolution layers using ActivationStats callback. It shows that the activations of the first convolution layer are all zeros, and it carries over the next layer, meaning that the model is not learning anything.\n\n\nCode\nfrom fastai.callback.hook import *\n# learn.recorder.plot_loss()\nlearn.activation_stats.plot_layer_stats(0)\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo fix this issue, we can try several techniques to improve the training stability of our model. ### Increase Batch Size To make training more stable, we can try to increase the batch size, because larger batch sizes prodive more accurate estimates of the gradients, which can help to reduce the noise in the training process. On the downside, larger batch sizes require more memory and less batches per epochs, which bring less opportunities for the model to update its weights, and also it is subject to hardware capabilities.\n\n\nCode\ndls = get_dls(bs=512)\nlearn = fit()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.333724\n0.179789\n0.946100\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStill, most of the activations are zeros, and the model is not learning anything when we change the batch size to 512 instead of 64.\n\n\nLearning Rate Finder\nIt is not favorable that we start training with a high learning rate for a bad initialization of weights. Also, we do not want to end training with a high learning rate either, because it can cause the model to overshoot the optimal weights.\nOne way to deal with this issue has been proposed by Leslie Smith in his paper Cyclical Learning Rates for Training Neural Networks (Smith, 2017)2. The idea is to use a learning rate that varies cyclically between a lower and upper bound during training. This allows the model to:\n\nExplore different regions of the loss landscape and can help to avoid getting stuck in local minima (higher training rate helps to skip over small local minima).\nImprove generalization. Based on the fact that the training model with high learning rate tends to have diverging loss. If it is trained with that high learning rate for a while and it can find a good loss, it will find an area that generalizes well. Thus, a good strategy is to start with a low learning rate, where the loss does not diverge, and then allow optimizer to find smoother areas of parameters by going to higher learning rates. When the smoother areas are found, we can bring the learning rate down again to refine the weights. (i.e. MomentumSGD A Disciplined Approach to Neural Network Hyper-Parameters: Part 1– LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY (Smith, 2017)3)\n\nIt is implemeted in fastai library as fit_one_cycle function.\n\n\nCode\ndef fit(epochs=1, lr=0.06):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit_one_cycle(epochs, lr)\n    return learn\n\nlearn = fit()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.221975\n0.078908\n0.975000\n00:06\n\n\n\n\n\nWe can view the learning rate schedule and momentum during training by plotting the learning rate using recorder.plot_sched function.\n\n\nCode\nlearn.recorder.plot_sched()\n\n\n\n\n\n\n\n\n\nFor fit_one_cycle function, there are several parameters that we can tune to improve the training stability and performance of our model, such as lr_max,pct_start, div_factor, and final_div_factor. For more details, see the fastai documentation.\nTo see what is happening to the activations of the penultimate convolution layer, we can plot the activation statistics again. Now, the percentage of dead activations (all zeros) is significantly reduced, and the model is finally learning.\n\n\n\n\n\n\n\n\n\nAs we paid attention to the activation statistics during training, near-zero activations appear ar the beginning of training and gradually decreases as the training progresses. It suggests that the model training is not smooth because of the cylical learning rate going up and down during the cycle.\nTo solve this problem, we can use batch normalization technique.\n\n\nBatch Normalization\nAs stated in Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Sergey Ioffe)4, they talked about the problem that we have seen earlier:\n\n“Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.”\n\nTo address this issue, they proposed a technique called batch normalization. &gt; “Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.”\nBatch normalization normalizes** the activations of each layer by averaging the means and standard deviations of the activations of a layer use those to normalize the activations. To further deal with situation when we need activation is high to make accurate prediction, they also introduced two learnable parameters per activation (i.e., \\(\\gamma\\) and \\(\\beta\\)), which are used to scale and shift the normalized activations. After normalization, the activations get a vector \\(y\\), and a batch normalization layer computes the output as follows: \\(\\gamma*y + \\beta\\)\nBy doing so, our activations can have any mean and standard deviation, which are independent from the previous layer.\n\n\n\n\n\n\nFigure 3: Batch Normalization\n\n\n\nTo add batch normalization to our simple_cnn, we can use nn.BatchNorm2d class from PyTorch. We can add a batch normalization layer after each convolution layer in our simple_cnn architecture as follows:\n\n\nCode\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  layers.append(nn.BatchNorm2d(nf))\n  if act: layers.append(nn.ReLU())\n  return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134114\n0.057422\n0.986200\n00:06\n\n\n\n\n\nAs a result, the accuracy is improved and the model is able to achieve around 98.6% accuracy on the MNIST dataset. Compared to the previous results, we observe that the model tends to generalize better with batch normalization. One possible reason is that batch normalization adds some noise to the activations during training, which can force the model learning more robust to these variations.\nAs some paper claimed that we should train with more epochs and larger learning rate when using batch normalization, we can try to train the model with 5 epochs and learning rate of 0.1.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.194603\n0.098067\n0.967400\n00:06\n\n\n1\n0.083216\n0.070085\n0.976700\n00:06\n\n\n2\n0.053030\n0.049988\n0.984000\n00:06\n\n\n3\n0.032268\n0.028644\n0.991300\n00:06\n\n\n4\n0.016631\n0.023865\n0.992200\n00:06\n\n\n\n\n\nGreat, at this point, the model is able to achieve around 99.2% accuracy on the digit recognition task on MNIST dataset, which is a significant improvement compared to the previous results."
  },
  {
    "objectID": "posts/convolution-neural-networks/index.html#footnotes",
    "href": "posts/convolution-neural-networks/index.html#footnotes",
    "title": "Building SoTA Convolution Neural Networks for Image Recognition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDumoulin, V. (2016). “A guide to convolution arithmetic for deep learning”.↩︎\nSmith, L. (2017). “Cyclical Learning Rates for Training Neural Networks”.↩︎\nSmith, L. (2017). “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1 LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY .↩︎\nSergey, Ioffe. (2015). “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.↩︎"
  }
]