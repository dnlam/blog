[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blog posts about Maths, AI/ML",
    "section": "",
    "text": "Recommendation System with Collaborative Filtering\n\n\n\nRecommendation\n\nCollaborative Filtering\n\nCode\n\n\n\n\n\n\n\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning a Language Model for Sentiment Classification\n\n\n\nNLP\n\nLanguage Model\n\nTransfer Learning\n\nCode\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/natural-language-processing/index.html",
    "href": "posts/natural-language-processing/index.html",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "",
    "text": "In this notebook, we are going to deep dive into natural language processing (NLP) using Deep Learning (info). Relying on the pretrained language model, we are going to fine-tune it to classify the reviews, which works as sentiment analysis, to categorize user reviews as bad/good ones.\nBased on a language model which has been trained before, we will apply transfer learning method for this task to transform prediction problem into classification problem. ::: {.callout-note} ## Language model In this blog post, we refer language model as a model which predicts the next word in a sentence given the previous words. It is a self-supervised learning task, where the model learns to predict the next word in a sentence based on the context provided by the preceding words. :::\n\n\n\n\n\n\nFigure 1: Transfer learning workflow for movie classifier\n\n\n\nAs shown in Figure 1, we will start with the Wikipedia language model with a dataset which so-called Wikitext103 1. Then, we are going to create an IMDb language model which predicts the next word of a movie reviews. This intermediate learning will help us to learn about IMDb-specific kinds of words like the name of actors and directors. Afterward, we end up with fine-tuning the language model for classification problem to classify reviews for a movie is good/bad.\n\n\n\n\n\n\nThree-Step Transfer Learning Process\n\n\n\n\nGet Pre-trained Model: Clone Wikitext103 language model\nDomain Adaptation: Fine-tune Wikitext103 which is based on Wiki texts with IMDb movie reviews.\n\nCreate Task-Specific: Refine the fine-tuned model for sentiment classification."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#text-preprocessing",
    "href": "posts/natural-language-processing/index.html#text-preprocessing",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\nIn order to build a language model with many complexities such as different sentence lengths in long documents, we can build a neural network model to deal with that issue. We apprehended that categorical variables (words) can be used as independent variables for a neural network (using embedding matrix). Then, we could do the same thing with text.\nFirst, we concatenate all the documents in our dataset into a big long string and split it into words. Our independent variables will be the sequence of words starting with the first word and ending with the second last, and our dependent variable would be the sequence of words starting with the second word and ending with the last words.\nIn our vocab, it might exist the very common words and new words. For new words, because we don’t have any pre-knowledge, so we will just initialize the corresponding row with a random vector.\nThese above steps can be listed as below: - Tokenization: convert the text into a list of words - Numericalization: make a list of all the unique words which appear, and convert each word into a number, by looking up its index in the vocab. - Language model data loader: handle creating dependant variables - Language model: handle input list by using recurrent neural network.\n\nTokenization\nBasically, tokenization converts the text into list of words. Firstly, we will grab our IMDb dataset and try out the tokenizer with all the text files.\n\n\nCode\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n# path.ls()\n\n\n\n\nCode\nfiles = get_text_files(path,folders=['train','test','unsup'])\n\n\nThe default English word tokenizer that FastAI used is called SpaCy which uses a sophisticated rules engine for particular words and URLs. Rather than directly using SpacyTokenizer, we are going to use WordTokenizer which always points to fastai’s current default word tokenizer.\n\n\nCode\ntxt = files[0].open().read()\ntxt[:60]\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\n\nprint(coll_repr(toks,30))\n\n\n(#365) ['While','the','premise','of','the','film','is','pretty','lame','(','Ollie','is','diagnosed','with','\"','hornophobia','\"',')',',','the','film','is','an','amiable','and','enjoyable','little','flick','.','It'...]\n\n\n\nSub-word tokenization\nIn additions to word tokenizer, sub-word tokenizer is really useful for languages which the spaces are not necessary for separations of components in a sentence (e.g: Chinese). To handle this, we will do 2 steps: - Analyze a corpus of documents to find the most commonly occurring groups of letters which form the vocab - Tokenize the corpus using this vocab of sub-word units\nFor example, we will first look into 2000 movie reviews:\n\n\nCode\ntxts = L(o.open().read() for o in files[:2000])\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nThen, the long underscore is when we replace the space and we can know where the sentences actually start and stop.\n\n\nCode\nsubword(10000)\n\n\n\n\n\n\n\n\n\n'▁Whil e ▁the ▁premise ▁of ▁the ▁film ▁is ▁pretty ▁lame ▁( O ll ie ▁is ▁diagnos ed ▁with ▁\" hor no pho b ia \") , ▁the ▁film ▁is ▁an ▁a mi able ▁and ▁enjoyable ▁little ▁flick . ▁It \\''\n\n\nIf we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence. So, there is a compromise to take into account when choosing sub-word vocab: A larger vocab means more fewer tokens per sentence which means faster training, less memory, less state for the model to remember, but it comes to the downside of larger embedding matrix and requiring more data to learn.\n\n\n\nNumericalization\nIn order to numericalize the tokens, we need to call setup first to create the vocab.\n\n\nCode\ntkn = Tokenizer(spacy)\ntoks300 = txts[:300].map(tkn)\ntoks300[0]\nnum = Numericalize()\nnum.setup(toks300)\ncoll_repr(num.vocab,20)\n\n\n\"(#2976) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\"\n\n\nThe results return our rule tokens first, and it is followed by word appearances, in frequency order. Once we created our Numerical object, we can use it as if it were a function.\n\n\nCode\nnums = num(toks)[:20]\nnums\n\n\nTensorText([   0,    9,  938,   14,    9,   30,   16,  173, 1227,   35,    0,\n              16,    0,   27,   23,    0,   23,   33,   10,    9])\n\n\n\n\nCode\n' '.join(num.vocab[o] for o in nums)\n\n\n'xxunk the premise of the film is pretty lame ( xxunk is xxunk with \" xxunk \" ) , the'\n\n\nNow, we have already had numerical data, we need to put them in batches for our model.\n\n\nProcessing Batches of texts\nRecalling the batch creation for the images when we have to reshape all the images to be same size before grouping them together in a single tensor for the efficient calculation purposes. It is a little bit different when dealing with texts because it is not desirable to resize the text length. Also, we want the model read texts in order so that it can efficiently predict what the next word is. This suggests that each new batch should begin precisely where the previous one left off.\nSo, the text stream will be cut into a certain number of batches (with batch size) with preserving the order of the tokens. Because we want the model to read continuous rows of the text.\nTo recap, at every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, that stream will be cut into a batch of fixed size consecutive mini stream. The model will read these mini streams in order and it will produce the same activation.\n\n\nCode\nnums300 = toks300.map(num)\ndl = LMDataLoader(nums300)\nx,y = first(dl)\nx.shape, y.shape\n\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\nThe batch size is 64x72. 64 is the default batch size and 72 is the default sequence length."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "href": "posts/natural-language-processing/index.html#create-a-language-model-using-datablock",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Create a language model using DataBlock",
    "text": "Create a language model using DataBlock\nBy default, fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock.\n\n\nCode\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\nblocks=TextBlock.from_folder(path, is_lm=True),\nget_items=get_imdb, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=128, seq_len=80)"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#fine-tuning-the-model",
    "href": "posts/natural-language-processing/index.html#fine-tuning-the-model",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Fine-tuning the model",
    "text": "Fine-tuning the model\nso far, we have finetuned encoder, which stores the trained parameter weights from previous step. Now, we are going to create a learner that load the finetuned encoder for fine-tuning. Then, we are going to fine-tune it over several epoch.\n\n\nCode\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n                                metrics=accuracy).to_fp16()\n\n\n\n\nCode\nlearn.load_encoder('finetuned')\n\n\n&lt;fastai.text.learner.TextLearner at 0x7f1c55cccbf0&gt;\n\n\nAs we are training a classification task, we only need to unfreeze several last layers of the encoder instead of unfreezing all the layers. By fitting the last layers first, we can adapt the model to the specific task without losing the general language understanding captured in the earlier layers. The result show that we achieve around 93 % accuracy, just with one cycle fitting.\n\n\nCode\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.219989\n0.168302\n0.937000\n09:15\n\n\n\n\n\nWe can also further unfreeze more layers and do training to see if the accuracy improves or not. It shows that the accuracy results have improved from 93% to approximately 94%.\n\n\nCode\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.198631\n0.152882\n0.942440\n11:43\n\n\n\n\n\nAs we can observe, the accuracy has improved with the unfreezing of more layers. Now, we unfreeze all layers and do some training. The accuracy results are further improved to approximately 95%.\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.160233\n0.147829\n0.945160\n13:34\n\n\n1\n0.155604\n0.143441\n0.947920\n13:35"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#key-points",
    "href": "posts/natural-language-processing/index.html#key-points",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Key points",
    "text": "Key points\n\nWe started with the Wikitext103 pretrained model and fine-tuned it on IMDb movie reviews, achieving approximately 35% accuracy in next-word prediction after 10 epochs of training.\nWe implemented a text preprocessing pipeline for a language model including:\n\nTokenization: Converting raw text into structured word sequences using SpaCy\nNumericalization: Mapping words to numerical representations for neural network processing\nBatch Creation: Organizing sequential text data for efficient model training\n\nWe achieved final sentiment classification through progressive layer unfreezing of fine-tuned model."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#technical-insights",
    "href": "posts/natural-language-processing/index.html#technical-insights",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Technical Insights",
    "text": "Technical Insights\n\n\n\n\n\n\nKey Technical Learnings\n\n\n\n\nModel Persistence: Demonstrated both FastAI and PyTorch approaches for saving and loading model states\nProgressive Training: Used gradual unfreezing technique to optimize classification performance"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#future-directions",
    "href": "posts/natural-language-processing/index.html#future-directions",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Future Directions",
    "text": "Future Directions\nThis foundation opens several avenues for enhancement:\n\nModel Architecture: Experiment with transformer-based models (BERT, GPT)\nDataset Expansion: Include additional movie review sources for robustness\nMulti-class Classification: Extend beyond binary sentiment to rating prediction\nReal-time Deployment: Package the model for production sentiment analysis"
  },
  {
    "objectID": "posts/natural-language-processing/index.html#final-thoughts",
    "href": "posts/natural-language-processing/index.html#final-thoughts",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis project demonstrates the power of transfer learning in NLP, showing how pretrained language models can be effectively adapted for specific downstream tasks. The combination of FastAI’s high-level API with PyTorch’s flexibility provides an excellent framework for both experimentation and production deployment."
  },
  {
    "objectID": "posts/natural-language-processing/index.html#footnotes",
    "href": "posts/natural-language-processing/index.html#footnotes",
    "title": "Fine-tuning a Language Model for Sentiment Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my blog!\nIn this site, you will find my contributions and highlights of my writings on Mathematics and Computer Science.\nIf you are interested in this field, feel free to connect with me and I would love to hear from you! See the article on contributing posts or my contact for additional details."
  },
  {
    "objectID": "posts/collaborative-filtering/index.html",
    "href": "posts/collaborative-filtering/index.html",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "",
    "text": "In this blog, we are going to dive into collaborative filtering which is a part of recommendation algorithms used in Netflix. Netflix may not know these particular properties of the films you watched, but it would be able to see that other people that watched the same movies could watch other movies that you are not watching yet. By applying this technique, Netflix can recommend us the contents of the movies that we have not watched before but relevant to what we liked, which the others too.\nCollaborative Filtering is a recommendation system that recommends items to users based on how other users with similar preferences and behaviors have interacted with the same item. It is based on the principle that similar users share similar interests and behaviors. Different to content-based filtering, which recommends items based on the features of the items themselves, collaborative filtering relies on the collective behavior of users to make recommendations.\nThe key idea of content-based filtering is based on latent factors which decides what kinds of movies you want to watch. ## Latent Factors"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#data-preparation",
    "href": "posts/collaborative-filtering/index.html#data-preparation",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Data preparation",
    "text": "Data preparation\nIndeed, we can not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can yous, called MovieLen which contains tens millions of movies ranking.\n\n\nCode\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\npath = untar_data(URLs.ML_100k)"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#key-points",
    "href": "posts/collaborative-filtering/index.html#key-points",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Key points",
    "text": "Key points"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#technical-insights",
    "href": "posts/collaborative-filtering/index.html#technical-insights",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Technical Insights",
    "text": "Technical Insights\n\n\n\n\n\n\n## Key Technical Learnings"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#future-directions",
    "href": "posts/collaborative-filtering/index.html#future-directions",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Future Directions",
    "text": "Future Directions"
  },
  {
    "objectID": "posts/collaborative-filtering/index.html#final-thoughts",
    "href": "posts/collaborative-filtering/index.html#final-thoughts",
    "title": "Recommendation System with Collaborative Filtering",
    "section": "Final Thoughts",
    "text": "Final Thoughts"
  }
]