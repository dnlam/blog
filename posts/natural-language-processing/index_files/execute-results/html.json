{
  "hash": "ae7bde9f2f8b6c81b2e5507de3d18e95",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Natural Language Processing\"\n# author: \"Harlow Malloc\"\ndate: \"2022-03-29\"\ncategories: [NLP, transfer learning, code]\nimage: \"NLP.png\"\nformat:\n  html:\n    code-fold: true\n    fig-cap-location: bottom\n    tbl-cap-location: top\n    toc: true\n    toc-depth: 3\n    toc-location: left\n    toc-title: \"Contents\"\ncrossref:\n  fig-title: \"Figure\"\n  eq-title: \"Equation\"\n  tbl-title: \"Table\"\njupyter: python3\nexecute:\n  cache: true\n  freeze: auto\n---\n\n# Objectives\nIn this notebook, we are going to deep dive into natural language processing (NLP) using Deep Learning ([info](https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d)). Relying on the pretrained language model, we are going to fine-tune it to classify the reviews, and it works as sentiment analysis. \n\nBased on a `language model` which has been trained to guess what the next word in the text is, we will apply transfer learning method for this NLP task.\n\n\n![Transfer learning workflow for movie classifier](https://github.com/fastai/fastbook/blob/master/images/att_00027.png?raw=1){#fig-transfer-learning fig-cap=\"Transfer learning workflow for movie classifier.\"}\n\nAs shown in @fig-transfer-learning, we will start with the Wikipedia language model with a subset which we called ``Wikitext103``. Then, we are going to create an IMDb language model which predicts the next word of a movie reviews. This intermediate learning will help us to learn about IMDb-specific kinds of words like the name of actors and directors. Afterward, we end up with our classifier. \n\n::: {.callout-tip}\n## Three-Step Transfer Learning Process\n1. **Pre-trained Model**: Start with Wikitext103 language model\n2. **Domain Adaptation**: Fine-tune on IMDb movie reviews  \n3. **Task-Specific**: Create sentiment classifier\n:::\n\n\n# Text Preprocessing\n\nIn order to build a language model with many complexities such as different sentence lengths in long documents, we can build a neural network model to deal with that issue. We apprehended that categorical variables (words) can be used as independent variables for a neural network (using embedding matrix).  Then, we could do the same thing with text.\n\nFirst, we concatenate all the documents in our dataset into a big long string and split it into words. Our independent variables will be the sequence of words starting with the first word and ending with the second last, and our dependent variable would be the sequence of words starting with the second word and ending with the last words. \n\nIn our vocab, it might exist the very common words and new words. For new words, because we don't have any pre-knowledge, so we will just initialize the corresponding row with a random vector. \n\nThese above steps can be listed as below:\n- Tokenization: convert the text into a list of words\n- Numericalization: make a list of all the unique words which appear, and convert each word into a number, by looking up its index in the vocab.\n- Language model data loader creation : handle creating dependant variables\n- Language model creation: handle input list by using recurrent neural network.\n\n## Tokenization\nBasically, tokenization convert the text into list of words. Firstly, we will grab our IMDb dataset and try out the tokenizer with all the text files.\n\n::: {#24a2f8f6 .cell message='false' execution_count=2}\n``` {.python .cell-code}\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n# path.ls()\n```\n:::\n\n\n::: {#b664c584 .cell message='false' execution_count=3}\n``` {.python .cell-code}\nfiles = get_text_files(path,folders=['train','test','unsup'])\n```\n:::\n\n\nThe default English word tokenizer that FastAI used is called `SpaCy` which uses a sophisticated riles engine for particular words and URLs. Rather than directly using ```SpacyTokenizer```, we are going to use ```WordTokenizer``` which always points to fastai's current default word tokenizer. \n\n::: {#218efb7a .cell message='false' execution_count=4}\n``` {.python .cell-code}\ntxt = files[0].open().read()\ntxt[:60]\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\n\nprint(coll_repr(toks,30))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(#365) ['While','the','premise','of','the','film','is','pretty','lame','(','Ollie','is','diagnosed','with','\"','hornophobia','\"',')',',','the','film','is','an','amiable','and','enjoyable','little','flick','.','It'...]\n```\n:::\n:::\n\n\n### Sub-word tokenization\nIn additions to word tokenizer, sub-word tokenizer is really useful for languages which the spaces are not necessary for separations of components in a sentence (e.g: Chinese). To handle this, we will do 2 steps:\n- Analyze a corpus of documents to find the most commonly occurring groups of letters which form the vocab\n- Tokenize the corpus using this vocab of sub-word units\n\nFor example, we will first look into 2000 movie reviews:\n\n::: {#eaee845a .cell message='false' execution_count=5}\n``` {.python .cell-code}\ntxts = L(o.open().read() for o in files[:2000])\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n```\n:::\n\n\nThen, the long underscore is when we replace the space and we can know where the sentences actually start and stop. \n\n::: {#902ae6ad .cell message='false' execution_count=6}\n``` {.python .cell-code}\nsubword(10000)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n'▁Whil e ▁the ▁premise ▁of ▁the ▁film ▁is ▁pretty ▁lame ▁( O ll ie ▁is ▁diagnos ed ▁with ▁\" hor no pho b ia \") , ▁the ▁film ▁is ▁an ▁a mi able ▁and ▁enjoyable ▁little ▁flick . ▁It \\''\n```\n:::\n:::\n\n\nIf we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence. So, there is a compromise to take into account when choosing sub-word vocab: A larger vocab means more fewer tokens per sentence which means faster training, less memory, less state for the model to remember, but it comes to the downside of larger embedding matrix and requiring more data to learn.\n\n## Numericalization\nIn order to numericalize the tokens, we need to call ```setup``` first to create the vocab. \n\n::: {#9c42c7c2 .cell message='false' execution_count=7}\n``` {.python .cell-code}\ntkn = Tokenizer(spacy)\ntoks300 = txts[:300].map(tkn)\ntoks300[0]\nnum = Numericalize()\nnum.setup(toks300)\ncoll_repr(num.vocab,20)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n\"(#2976) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\"\n```\n:::\n:::\n\n\nThe results return our rule tokens first, and it is followed by word appearances, in frequency order.\nOnce we created our Numerical object, we can use it as if it were a function.\n\n::: {#12ff0b7e .cell execution_count=8}\n``` {.python .cell-code}\nnums = num(toks)[:20]\nnums\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nTensorText([   0,    9,  938,   14,    9,   30,   16,  173, 1227,   35,    0,\n              16,    0,   27,   23,    0,   23,   33,   10,    9])\n```\n:::\n:::\n\n\n::: {#487d2ed6 .cell execution_count=9}\n``` {.python .cell-code}\n' '.join(num.vocab[o] for o in nums)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n'xxunk the premise of the film is pretty lame ( xxunk is xxunk with \" xxunk \" ) , the'\n```\n:::\n:::\n\n\nNow, we have already had numerical data, we need to put them in batches for our model.\n\n### Batches of texts\nRecalling the batch creation for the images when we have to reshape all the images to be same size before grouping them together in a single tensor for the efficient calculation purposes. It is a little bit different when dealing with texts because it is not desirable to resize the text length. Also, we want the model read texts in order so that it can efficiently predict what the next word is. This suggests that each new batch should begin precisely where the previous one left off.\n\nSo, the text stream will be cut into a certain number of batches (with batch size) with preserving the order of the tokens. Because we want the model to read continuous rows of the text.\n\nTo recap, at every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, that stream will be cut into a batch of fixed size consecutive mini stream. The model will read these mini streams in order and it will produce the same activation.\n\n::: {#075c06e5 .cell execution_count=10}\n``` {.python .cell-code}\nnums300 = toks300.map(num)\ndl = LMDataLoader(nums300)\nx,y = first(dl)\nx.shape, y.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n(torch.Size([64, 72]), torch.Size([64, 72]))\n```\n:::\n:::\n\n\nThe batch size is 64x72. 64 is the default batch size and 72 is the default sequence length.\n\n# Training a Text Classifier\n## Create a language model using DataBlock\nBy default, `fastai` handles tokenization and numericalization automatically when `TextBlock` is passed to `DataBlock`. \n\n::: {#67d8a933 .cell execution_count=11}\n``` {.python .cell-code}\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\nblocks=TextBlock.from_folder(path, is_lm=True),\nget_items=get_imdb, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=128, seq_len=80)\n```\n:::\n\n\n## Fine-tuning the language model\n\nIn this step, we are going to create a learner which is going to learn and predict the next word of a `movie review`. It will take the data from data loader, pretrained model (``AWD_LSTM``), apply dropout technique and take accuracy as well as perplexity metrics into account. Particularly, `accuracy` metric is used to evaluate how the correctness when the model tries to predict the next word, while `perplexity` metric is used to track the (exponential) value of cross-entropy loss.\n\n::: {#b5c2a176 .cell execution_count=12}\n``` {.python .cell-code}\nlearn = language_model_learner(\ndls_lm, AWD_LSTM, drop_mult=0.3,\nmetrics=[accuracy, Perplexity()]).to_fp16()\n```\n:::\n\n\nThen, we will perform intermediate model training by fitting the model in one training cycle. \n\n::: {#6f8bddf1 .cell cache='true' message='false' execution_count=13}\n``` {.python .cell-code}\nlearn.fit_one_cycle(1,2e-2)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>4.022653</td>\n      <td>3.895626</td>\n      <td>0.300949</td>\n      <td>49.186825</td>\n      <td>2:13:57</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nAfter few minutes of training, we got the prediction accuracy which is around 29-30 percent.\nIn order to reuse the pre-trained model, we can easily save the model with PyTorch. In this case, we are going to save only learnable parameters (i.e., weight and bias of a model via `state_dict`) and the updated parameters after one epoch training will be stored at `learn.path/'models'/'one_epoch_training_torch.pth'`.\n\n::: {#94ab1d5c .cell execution_count=14}\n``` {.python .cell-code}\n# Option 1: Save with FastAI\n# learn.save('one_epoch_training')\n\n# Option 2: Save with PyTorch\nimport torch\nmodel_save_path = learn.path/'models'/'one_epoch_training_torch.pth'\ntorch.save(learn.model.state_dict(), model_save_path)\n# print(f\"Model saved to: {model_save_path}\")\n```\n:::\n\n\nOnce the trainable parameters are stored, we can later load those parameter to the compatible model for further training \n\n::: {.callout-warning}\n## Implementation Note: PyTorch Model Loading\nWhen using `torch.load()`, be cautious about the `weights_only` parameter. For security reasons, consider using `weights_only=True` when loading models from untrusted sources to prevent execution of arbitrary code.\n:::\n\n::: {#0834ecf2 .cell execution_count=15}\n``` {.python .cell-code}\n# Option 1: Use FastAI's load method\n# learn.load('one_epoch_training', strict=False)\n\n# Option 2: Use PyTorch to load the saved model\nimport torch\nmodel_load_path = learn.path/'models'/'one_epoch_training_torch.pth'\nstate_dict = torch.load(model_load_path, weights_only=False)\nlearn.model.load_state_dict(state_dict, strict=False)\n# print(f\"Model loaded from: {model_load_path}\")\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<All keys matched successfully>\n```\n:::\n:::\n\n\nAfter loading the pre-saved model, we can unfreeze it and train it for few more epochs. Then, let's see the improvement of the accuracy.\n\n::: {#56ab7ee6 .cell cache='true' message='false' execution_count=16}\n``` {.python .cell-code}\nlearn.unfreeze()\n\nlearn.fit_one_cycle(10,2e-3)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.756103</td>\n      <td>3.753208</td>\n      <td>0.317186</td>\n      <td>42.657707</td>\n      <td>2:09:41</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3.701586</td>\n      <td>3.692517</td>\n      <td>0.324204</td>\n      <td>40.145767</td>\n      <td>2:10:06</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.631865</td>\n      <td>3.643081</td>\n      <td>0.329702</td>\n      <td>38.209377</td>\n      <td>2:09:27</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.557527</td>\n      <td>3.611756</td>\n      <td>0.333258</td>\n      <td>37.031033</td>\n      <td>2:10:08</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.496033</td>\n      <td>3.588261</td>\n      <td>0.336209</td>\n      <td>36.171108</td>\n      <td>2:09:46</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.427615</td>\n      <td>3.574450</td>\n      <td>0.338256</td>\n      <td>35.675011</td>\n      <td>2:09:23</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.356645</td>\n      <td>3.564668</td>\n      <td>0.339928</td>\n      <td>35.327736</td>\n      <td>2:08:22</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.309995</td>\n      <td>3.562580</td>\n      <td>0.340825</td>\n      <td>35.254047</td>\n      <td>2:08:05</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.255192</td>\n      <td>3.565229</td>\n      <td>0.341031</td>\n      <td>35.347553</td>\n      <td>2:06:47</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.210757</td>\n      <td>3.569845</td>\n      <td>0.340811</td>\n      <td>35.511105</td>\n      <td>2:05:18</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nAs we can see from the training process, the accuracy has improved progressively. At the end of ten cycle training, the accuracy has increased to around 35 percent. \nTo perform model finetuning, we save the model parameters except the last activation function layer. To do that, we can save it with `save_encoder`\n\n::: {#0136d40f .cell execution_count=17}\n``` {.python .cell-code}\nlearn.save_encoder('finetuned')\n```\n:::\n\n\nIn this step, we have fine tuned the language model. Now, we will fine tune this language model using the IMDb sentiment labels.\n\nAlthough the model is pre-designed for next word prediction, we can also use this model to generate texts. For example, we can self-create a sentence with some words and we parses this sentence to the model to generate a new sentence which has one word longer than the parsed sentence. Leveraging this capability, we are going to create 40 new words from that randomized content.\n\n::: {#7f8c06d1 .cell message='false' execution_count=18}\n``` {.python .cell-code}\nTEXT = \"I liked this movie so\"\n\nN_WORDS = 40\n\nN_SENTENCES = 2\n\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n:::\n\n\nLet's see the generation of new inventing words\n\n::: {#e52559e2 .cell execution_count=19}\n``` {.python .cell-code}\nprint(\"\\n\".join(preds))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ni liked this movie so much i bought it on ebay and so i bought it . This is not a very good movie but you do have to pay attention to the story , there is so much that is true about the\ni liked this movie so much . The acting was good , the camera work was good , the effects were good . i was also surprised to see a few of the actors who played Mr . Thomas Gomez were\n```\n:::\n:::\n\n\n## Fine-tuning the classification model\n\nPreviously, we built a language model to predict the next word of a document given the input text. Now, we are going to move to the classifier which predicts the sentiment of a document.\n\n::: {#e985ec54 .cell execution_count=20}\n``` {.python .cell-code}\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n```\n:::\n\n\nIn the `TextBlock.from_folder()` function, we do not set `is_lm=True` because we tell `TextBlock` that we had regular labelled data rather than using next word as a label as we did for prediction.\n\n::: {.callout-important}\n## Create training batch for sentiment classification\nIt is important to say that  we need to collate all the items in a batch into a single tensor, and a tensor has a fixed size. Therefore, we need to pad/crop/squish our sequences to make the inputs have the same length. \nFor the characteristics of the input, we will apply padding here.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}