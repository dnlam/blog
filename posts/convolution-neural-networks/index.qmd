---
title: "Building SoTA Convolution Neural Networks for Image Recognition"
# author: "Lam Dinh"
date: "2022-05-10"
categories: [CNN, code]
image: "resnet.png"
pdf-engine: pdflatex
bibliography: references.bib
format:
  html:
    code-fold: true
    fig-cap-location: bottom
    tbl-cap-location: top
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
crossref:
  fig-title: "Figure"
  eq-title: "Equation"
  tbl-title: "Table"
jupyter: python3
execute: 
  cache: true
  freeze: auto
---
```{python}
#| echo: false
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: true
from fastai.vision.all import *
from fastai.vision.models import *
import pandas as pd
import numpy as np
import torch.nn as nn
import torch.nn.functional as F

path_imagenette=untar_data(URLs.IMAGENETTE)
path_mnist=untar_data(URLs.MNIST_SAMPLE)
im3 = Image.open(path_mnist/'train'/'3'/'12.png')
show_image(im3)
```

# Overview

In this blog post, we will explore the architecture of Convolution Neural Networks (CNN) and how they have been used to achieve state-of-the-art performance in image recognition tasks. We will also discuss some of the key components of CNNs, such as convolution layers, pooling layers, and activation functions. Finally, we will look at one of the most popular CNN architectures: `ResNet`.

# Overview of Convolution Neural Networks (CNN)
In the context of computer vision, **feature engineering** is the process of using domain knowledge to extract distinctive attributes from images that can be used to improve the performance of machine learning algorithms. For instance, in image classification tasks, the number 7 is characterized by a horizontal edge near the top, and a diagonal line that goes down to the right. These features can be used to distinguish the number 7 from other digits.

It turns out that finding the edges in an image is a crucial step in computer vision tasks. To achieve this, we can use a technique called **convolution**. Convolution is a mathematical operation that takes two inputs: an image and a filter (also known as a kernel). The filter is a small matrix that is used to scan the image and extract features. For example, the following filter can be used to detect horizontal edges in an image. 

## Convolution Layer
A convolution layer applies a set of filters (i.e., **kernel**) to the input image to extract features. Each filter/kernel is a small matrix that is used to scan the image and extract features. The output of a convolution layer is a set of feature maps, which are the result of applying each filter to the input image.

![An example of kernel](./imgs/kernel.png){#fig-kernel fig-align="center"}

As illustrated in Figure @fig-kernel, a 3x3 matrix kernel is applied to the input image, which is 7x7 grid. The kernel is applied to each pixel in the image, and the output is a new pixel value that is calculated by taking the dot product of the kernel and the corresponding pixels in the image. This process is repeated for each pixel in the image, resulting in a new feature map.

Let's take another look at how convolution works in practice. We will use the `im3` image, which is a 28x28 grayscale image of the digit 3 from the MNIST dataset. We will apply a 3x3 kernel to the image to extract features.

```{python}
#| echo: false
#| output: true
#| eval: true
#| warning: false
#| message: false
#| cache: true

im3_t=tensor(im3)
df = pd.DataFrame(im3_t[:10,:30])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
```

Let's define a kernel that detects horizontal edges in the image. The kernel is a 3x3 matrix with values that are designed to highlight horizontal edges.
```{python}
kernel = tensor([[-1., -1., -1.],
                 [ 0.,  0.,  0.],
                 [ 1.,  1.,  1.]]).float()
```

This kernel will detect horizontal edges in the image by emphasizing the differences between the pixel values in the top and bottom rows of the kernel, we can also change the kernel to have the row of 1s at the top and -1s at the bottom, we can detect horizontal edges that go from dark to light, putting 1s and -1s in columns versus rows give us filters that detect vertical edges.  

```{python}
def apply_kernel(row, col, kernel):
    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()
```

For a more in-depth guide to convolution arithmetic, see the paper [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) (Dumoulin, 2013)^[Dumoulin, V. (2016). "A guide to convolution arithmetic for deep learning".].


## Strides and Padding
With convolution arithmetic, the kernel is applied to each pixel in the image, resulting in a new feature map that the dimensions are smaller than the original image. This is because the kernel cannot be applied to the pixels at the edges of the image. To address this issue, we can use two techniques: **strides** and **padding**.

**Strides** refer to the number of pixels by which we move the kernel across the image. By default, the stride is set to 1, meaning we move the kernel one pixel at a time. However, we can increase the stride to reduce the size of the output feature map. For example, if we set the stride to 2, the kernel will move two pixels at a time, resulting in a smaller output feature map.

**Padding** involves adding extra pixels around the edges of the image before applying the kernel. This allows us to preserve the spatial dimensions of the input image in the output feature map. There are different types of padding, such as zero-padding (adding zeros) and reflection padding (adding a mirror image of the border pixels).

![An example of padding with stride of 2](./imgs/padding.png){#fig-padding fig-align="center"}

As illustrated in @fig-padding, a 5x5 input image is padded with a 2-pixel border of zeros, resulting in a 7x7 padded image. A 4x4 kernel is then applied to the padded image with a stride of 1, resulting in a 5x5 output feature map.


In general, if we add a kernel of size $ks \times ks$ ($ks$ is an odd number) to an input image of size $n \times n$, the neccessary padding $p$ to preserve the spatial dimensions of the input image in the output feature map is given by:
$$
p = ks//2
$$
When $ks$ is even, we can use asymmetric padding, for example, if $ks=4$, we can use $p=(ks//2, ks//2-1)$. 

Furthermore, if we apply the kernel with a stride of $s$, the output feature map will have dimensions:
$$
\text{output size} = (n + 2p - ks)//(s) + 1
$$

## Create a Convolution Layer with PyTorch
We can create a convolution layer using PyTorch's `nn.Conv2d` class. The `nn.Conv2d` class takes several parameters, including the number of input channels, the number of output channels, the kernel size, the stride, and the padding.

```{python}
#| echo: false
#| output: true
#| eval: true
#| warning: false
#| message: false
#| cache: true

cnn = sequential(
    nn.Conv2d(1, 30, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.Conv2d(30, 1, kernel_size=3, stride=1, padding=1)
)
```

In this example, we create a convolution layer with 1 input channel (grayscale image), 30 output channels (feature maps), a kernel size of 3x3, a stride of 1, and padding of 1. We also apply the ReLU activation function after the first convolution layer.
One interesting property to note here is that we do not need to specify the input size when creating the convolution layer because a convolution is applied over each pixel automatically.

```{python}
#| echo: false
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: true

mnist = DataBlock(
    blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),
    get_items=get_image_files,
    splitter=GrandparentSplitter(),
    get_y=parent_label,
    # item_tfms=Resize(28),
    # batch_tfms=Normalize()
)

dls = mnist.dataloaders(path_mnist)
xb,yb = first(dls.valid) 
```

When creating `cnn` as above, we see that the output shape is the same as the input shape, which is (28, 28) (This is because we have used padding to preserve the spatial dimensions of the input image in the output feature map). It is not interesting for classification task since we need only single output activation per input image. 

To deal with this, we can use several stride-2 convolution layers to reduce the spatial dimensions of the input image in the output feature map. For example, we can use two stride-2 convolution layers to reduce the spatial dimensions of the input image from (28, 28) to (7, 7), (4x4), (2x2) and then 1.

```{python}
def conv(ni, nf, ks=3, act=True):
    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)
    if act: res = nn.Sequential(res, nn.ReLU()) 
    return res
```

Then, we create a simple `cnn` which consists of several convolution layers with stride of 2, kernel size of 3 to reduce the spatial dimensions of the input image in the output feature map. We also flatten the output feature map before passing it to the final classification layer.:

```{python}
simple_cnn = sequential(
    conv(1, 4),   # Input: 28x28 -> Output: 14x14
    conv(4, 8),  # Input: 14x14 -> Output: 7x7
    conv(8, 16), # Input: 7x7 -> Output: 4x4
    conv(16, 32), # Input: 4x4 -> Output: 2x2
    conv(32, 2, act=False), # Input: 2x2 -> Output: 1x1
    Flatten()
)
```

To test our `simple_cnn`, we can train a classification model from a batch of images from the MNIST dataset to see how effective of the feature extraction it is. To do this, we build a `Learner` from `simple_cnn` and dataset `dls` as follows:

```{python}
learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)
learn.summary()
```

As we can see, the output of the final Conv2D layer is 64x2x1x1, that's why we need to flatten it before passing it to the final classification layer.

Afterwards, let's train the model with low learning rate and 2 epochs using `fit_one_cycle` function.

```{python}
learn.fit_one_cycle(2, 1e-2)
```

Impressive, we are able to achieve over 98% accuracy on the classification task with MNIST dataset using simple CNN architecture (built from scratch).

## Improving Training Stability
So far, we have created a simple 2D CNN for image classification task over the MNIST dataset and achieved around 98% accuracy. In this section, we will talk about several techniques that we can use to improve the training stability and performance of our model. To make it more interesting, we will train a CNN model to recognize 10 digits from the MNIST dataset and apply several techniques to improve its performance.

```{python}
#| echo: false
#| output: true
#| eval: true
#| warning: false
#| message: false
#| cache: true
# path_mnist.ls()
path= untar_data(URLs.MNIST)
def get_dls(bs=64):
  return DataBlock(
      blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),
      get_items=get_image_files,
      splitter=GrandparentSplitter('training', 'testing'),
      get_y=parent_label,
      # item_tfms=Resize(28),
      batch_tfms=Normalize()
  ).dataloaders(path, bs=bs)
dls = get_dls()
dls.show_batch(max_n=9, figsize=(4,4))
```

### Use more activation functions
One simple tweak that we can apply to improve recognition accuracy is to use more activation functions in our `CNN`, as we need more `filters` to learn more complex patterns in 10-digit MNIST samples. To achieve this, we add one more activation function after each convolution layer in our `simple_cnn` architecture. As a result, the number of activations ends up being doubled.

However, adding more activation functions can lead to a subtle (training) problem. Specifically, when we apply 3x3-pixel kernel to the first convolution layer with 4 output filters, we embed information from 9 input pixels into 4 output pixels. While doubling the number of activation functions, we have the computation of 8 output pixels from 9 input pixels. It makes neural networks more difficult to learn the features while mapping from 9 input pixels to 8 output pixels than from 9 input pixels to 4 output pixels.

To deal with this issue, we can increase the kernel size from 3 to 5, which allows us to embed information from 25 input pixels into 8 output pixels. This makes it easier for the neural network to learn the features while mapping from 25 input pixels to 8 output pixels.


```{python}
def simple_cnn():
  return sequential(
    conv(1, 8, 5),   # Input: 28x28 -> Output: 14x14
    conv(8, 16),  # Input: 14x14 -> Output: 7x7
    conv(16, 32), # Input: 7x7 -> Output: 4x4
    conv(32, 64), # Input: 4x4 -> Output: 2x2
    conv(64, 10, act=False), # Input: 2x2 -> Output: 1x1
    Flatten()
)
```

To train the model more quickly, we can set learning rate to 0.06 and use `ActivationStats` callback to monitor the activation statistics during training.
```{python}
def fit(epochs=1):
    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))
    learn.fit(epochs, 0.06)
    return learn

learn = fit()
```
Surprisingly, the model is not trained at all, the accuracy is just around 10% (random guess). 
To findout what went wrong, we can plot the activation statistics of the first/penultimate convolution layers using `ActivationStats` callback. 
It shows that the activations of the first convolution layer are all zeros, and it carries over the next layer, meaning that the model is not learning anything.
```{python}
from fastai.callback.hook import *
# learn.recorder.plot_loss()
learn.activation_stats.plot_layer_stats(0)
learn.activation_stats.plot_layer_stats(-2)
```
To fix this issue, we can try several techniques to improve the training stability of our model.
### Increase Batch Size
To make training more stable, we can try to increase the batch size, because larger batch sizes prodive more accurate estimates of the gradients, which can help to reduce the noise in the training process. On the downside, larger batch sizes require more memory and less batches per epochs, which bring less opportunities for the model to update its weights, and also it is subject to hardware capabilities.

```{python}
dls = get_dls(bs=512)
learn = fit()
```

```{python}
#| echo: false
learn.activation_stats.plot_layer_stats(-2)
```

Still, most of the activations are zeros, and the model is not learning anything when we change the batch size to 512 instead of 64.

### Learning Rate Finder

It is not favorable that we start training with a high learning rate for a bad initialization of weights. Also, we do not want to end training with a high learning rate either, because it can cause the model to overshoot the optimal weights. 

One way to deal with this issue has been proposed by Leslie Smith in his paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1708.07120) (Smith, 2017)^[Smith, L. (2017). "Cyclical Learning Rates for Training Neural Networks".]. The idea is to use a learning rate that varies cyclically between a lower and upper bound during training. This allows the model to:

- Explore different regions of the loss landscape and can help to avoid getting stuck in local minima (higher training rate helps to skip over small local minima).

- Improve generalization. Based on the fact that the training model with high learning rate tends to have diverging loss. If it is trained with that high learning rate for a while and it can find a good loss, it will find an area that generalizes well. Thus, a good strategy is to start with a low learning rate, where the loss does not diverge, and then allow optimizer to find smoother areas of parameters by going to higher learning rates. When the smoother areas are found, we can bring the learning rate down again to refine the weights. (i.e. `MomentumSGD` [A Disciplined Approach to Neural Network Hyper-Parameters: Part 1– LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY](https://arxiv.org/pdf/1803.09820) (Smith, 2017)^[Smith, L. (2017). "A Disciplined Approach to Neural Network Hyper-Parameters: Part  1 LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY .])


It is implemeted in `fastai` library as `fit_one_cycle` function.

```{python}
def fit(epochs=1, lr=0.06):
    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))
    learn.fit_one_cycle(epochs, lr)
    return learn

learn = fit()
```

We can view the learning rate schedule and momentum during training by plotting the learning rate using `recorder.plot_sched` function.
```{python}
learn.recorder.plot_sched()
```

For `fit_one_cycle` function, there are several parameters that we can tune to improve the training stability and performance of our model, such as `lr_max`,`pct_start`, `div_factor`, and `final_div_factor`. For more details, see the [fastai documentation](https://docs.fast.ai/callback.schedule.html#fastai.callback.schedule.fit_one_cycle).


To see what is happening to the activations of the penultimate convolution layer, we can plot the activation statistics again. Now, the percentage of dead activations (all zeros) is significantly reduced, and the model is finally learning.
```{python}
#| echo: false
learn.activation_stats.plot_layer_stats(-2)
```

As we paid attention to the activation statistics during training, near-zero activations appear ar the beginning of training and gradually decreases as the training progresses. It suggests that the model training is not smooth because of the cylical learning rate going up and down during the cycle. 

To solve this problem, we can use batch normalization technique.

### Batch Normalization
As stated in [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) (Sergey Ioffe)^[Sergey, Ioffe. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.], they talked about the problem that we have seen earlier:

> "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities."

To address this issue, they proposed a technique called **batch normalization**.
> "Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization." 


**Batch normalization** normalizes** the activations of each layer by averaging the means and standard deviations of the activations of a layer use those to normalize the activations. To further deal with situation when we need activation is high to make accurate prediction, they also introduced two learnable parameters per activation (i.e., $\gamma$ and $\beta$), which are used to scale and shift the normalized activations. After normalization, the activations get a vector $y$, and a batch normalization layer computes the output as follows: $\gamma*y + \beta$

By doing so, our activations can have any mean and standard deviation, which are independent from the previous layer. 

![Batch Normalization ](./imgs/batch_norm.png){#fig-bn fig-align="center"}

To add batch normalization to our `simple_cnn`, we can use `nn.BatchNorm2d` class from PyTorch. We can add a batch normalization layer after each convolution layer in our `simple_cnn` architecture as follows:

```{python}
def conv(ni, nf, ks=3, act=True):
  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]
  layers.append(nn.BatchNorm2d(nf))
  if act: layers.append(nn.ReLU())
  return nn.Sequential(*layers)
```


```{python}
#| echo: false
learn=fit()
```
As a result, the accuracy is improved and the model is able to achieve around 98.6% accuracy on the MNIST dataset. Compared to the previous results, we observe that the model tends to generalize better with batch normalization. One possible reason is that batch normalization adds some noise to the activations during training, which can force the model learning more robust to these variations.

As some paper claimed that we should train with more epochs and larger learning rate when using batch normalization, we can try to train the model with 5 epochs and learning rate of 0.1.

```{python}
#| echo: false
learn=fit(5, 0.1)
```

Great, at this point, the model is able to achieve around 99.2% accuracy on the digit recognition task on MNIST dataset, which is a significant improvement compared to the previous results.

# Residual Networks (ResNet)

# Conclusions
